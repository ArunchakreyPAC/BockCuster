{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **Cleaning Weather datasets**",
   "id": "c86efbe7e7b961a9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T11:00:57.771421Z",
     "start_time": "2024-09-28T11:00:57.760368Z"
    }
   },
   "cell_type": "code",
   "source": "import pandas as pd",
   "id": "f312b5fa98267b6a",
   "outputs": [],
   "execution_count": 135
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T11:00:57.918380Z",
     "start_time": "2024-09-28T11:00:57.808683Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "weather_data_csv = 'datasets/weatherAUS.csv'\n",
    "processed_data_csv = 'datasets/processed_weatherAUS.csv'\n",
    "\n",
    "# weather_data = pd.read_csv(weather_data_csv)\n",
    "processed_data = pd.read_csv(processed_data_csv)\n",
    "\n",
    "# print(weather_data.columns.tolist())\n",
    "\n",
    "# weather_data.drop(columns=['Evaporation',\n",
    "#                            'WindGustDir',\n",
    "#                            'WindGustSpeed',\n",
    "#                            'WindDir9am',\n",
    "#                            'WindDir3pm',\n",
    "#                            'Temp9am',\n",
    "#                            'Temp3pm',\n",
    "#                            'RainToday',\n",
    "#                            'RainTomorrow'], inplace=True)\n",
    "\n",
    "processed_data['Location'].unique()"
   ],
   "id": "29ed50f9c7a20a4f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Albury', 'BadgerysCreek', 'Cobar', 'CoffsHarbour', 'Moree',\n",
       "       'Newcastle', 'NorahHead', 'NorfolkIsland', 'Penrith', 'Richmond',\n",
       "       'Sydney', 'SydneyAirport', 'WaggaWagga', 'Williamtown',\n",
       "       'Wollongong', 'Canberra', 'Tuggeranong', 'MountGinini', 'Ballarat',\n",
       "       'Bendigo', 'Sale', 'MelbourneAirport', 'Melbourne', 'Mildura',\n",
       "       'Nhil', 'Portland', 'Watsonia', 'Dartmoor', 'Brisbane', 'Cairns',\n",
       "       'GoldCoast', 'Townsville', 'Adelaide', 'MountGambier', 'Nuriootpa',\n",
       "       'Woomera', 'Albany', 'Witchcliffe', 'PearceRAAF', 'PerthAirport',\n",
       "       'Perth', 'SalmonGums', 'Walpole', 'Hobart', 'Launceston',\n",
       "       'AliceSprings', 'Darwin', 'Katherine', 'Uluru'], dtype=object)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 136
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **Reading the flu file**",
   "id": "593ca44b0ae8180f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T11:00:58.365188Z",
     "start_time": "2024-09-28T11:00:58.036099Z"
    }
   },
   "cell_type": "code",
   "source": [
    "flu_path = 'datasets/people_per_date.csv'\n",
    "\n",
    "flu_data = pd.read_csv(flu_path)\n",
    "\n",
    "# Convert the data column into datetime format\n",
    "processed_data['Date'] = pd.to_datetime(processed_data['Date'])\n",
    "\n",
    "# Create yearweek column (the numbered week in the year)\n",
    "processed_data['Year_Week'] = processed_data['Date'].dt.strftime('%Y-%U')\n",
    "flu_data['Week Ending (Friday)'] = pd.to_datetime(flu_data['Week Ending (Friday)'])\n",
    "flu_data['Year_Week'] = flu_data['Week Ending (Friday)'].dt.strftime('%Y-%U')\n",
    "\n",
    "flu_data['Daily_Flu_Cases'] = flu_data['Number of Sick People'] / 7\n",
    "\n",
    "# merging the flu dataset with the weather dataset\n",
    "merged_data = pd.merge(processed_data, flu_data[['Year_Week', 'Daily_Flu_Cases']], on='Year_Week', how='left')\n",
    "\n",
    "merged_data"
   ],
   "id": "d9d9f61e74086d23",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "             Date Location  MinTemp  MaxTemp  Rainfall  HumidityAve  \\\n",
       "0      2008-12-01   Albury     13.4     22.9       0.6         46.5   \n",
       "1      2008-12-02   Albury      7.4     25.1       0.0         34.5   \n",
       "2      2008-12-03   Albury     12.9     25.7       0.0         34.0   \n",
       "3      2008-12-04   Albury      9.2     28.0       0.0         30.5   \n",
       "4      2008-12-05   Albury     17.5     32.3       1.0         57.5   \n",
       "...           ...      ...      ...      ...       ...          ...   \n",
       "145455 2017-06-21    Uluru      2.8     23.4       0.0         37.5   \n",
       "145456 2017-06-22    Uluru      3.6     25.3       0.0         38.5   \n",
       "145457 2017-06-23    Uluru      5.4     26.9       0.0         38.5   \n",
       "145458 2017-06-24    Uluru      7.8     27.0       0.0         37.5   \n",
       "145459 2017-06-25    Uluru     14.9      NaN       0.0         49.0   \n",
       "\n",
       "        WindSpeedAve  PressureAve  CloudAve  WindGustSpeed Year_Week  \\\n",
       "0               22.0      1007.40       NaN           44.0   2008-48   \n",
       "1               13.0      1009.20       NaN           44.0   2008-48   \n",
       "2               22.5      1008.15       NaN           46.0   2008-48   \n",
       "3               10.0      1015.20       NaN           24.0   2008-48   \n",
       "4               13.5      1008.40       7.5           41.0   2008-48   \n",
       "...              ...          ...       ...            ...       ...   \n",
       "145455          12.0      1022.45       NaN           31.0   2017-25   \n",
       "145456          11.0      1021.30       NaN           22.0   2017-25   \n",
       "145457           9.0      1018.90       NaN           37.0   2017-25   \n",
       "145458          10.0      1017.95       2.5           28.0   2017-25   \n",
       "145459          17.0      1019.05       8.0            NaN   2017-26   \n",
       "\n",
       "        Daily_Flu_Cases  \n",
       "0              8.428571  \n",
       "1              8.428571  \n",
       "2              8.428571  \n",
       "3              8.428571  \n",
       "4              8.428571  \n",
       "...                 ...  \n",
       "145455       299.142857  \n",
       "145456       299.142857  \n",
       "145457       299.142857  \n",
       "145458       299.142857  \n",
       "145459       432.142857  \n",
       "\n",
       "[145460 rows x 12 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Location</th>\n",
       "      <th>MinTemp</th>\n",
       "      <th>MaxTemp</th>\n",
       "      <th>Rainfall</th>\n",
       "      <th>HumidityAve</th>\n",
       "      <th>WindSpeedAve</th>\n",
       "      <th>PressureAve</th>\n",
       "      <th>CloudAve</th>\n",
       "      <th>WindGustSpeed</th>\n",
       "      <th>Year_Week</th>\n",
       "      <th>Daily_Flu_Cases</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008-12-01</td>\n",
       "      <td>Albury</td>\n",
       "      <td>13.4</td>\n",
       "      <td>22.9</td>\n",
       "      <td>0.6</td>\n",
       "      <td>46.5</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1007.40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>44.0</td>\n",
       "      <td>2008-48</td>\n",
       "      <td>8.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2008-12-02</td>\n",
       "      <td>Albury</td>\n",
       "      <td>7.4</td>\n",
       "      <td>25.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.5</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1009.20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>44.0</td>\n",
       "      <td>2008-48</td>\n",
       "      <td>8.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2008-12-03</td>\n",
       "      <td>Albury</td>\n",
       "      <td>12.9</td>\n",
       "      <td>25.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>22.5</td>\n",
       "      <td>1008.15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>46.0</td>\n",
       "      <td>2008-48</td>\n",
       "      <td>8.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2008-12-04</td>\n",
       "      <td>Albury</td>\n",
       "      <td>9.2</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.5</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1015.20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2008-48</td>\n",
       "      <td>8.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2008-12-05</td>\n",
       "      <td>Albury</td>\n",
       "      <td>17.5</td>\n",
       "      <td>32.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>57.5</td>\n",
       "      <td>13.5</td>\n",
       "      <td>1008.40</td>\n",
       "      <td>7.5</td>\n",
       "      <td>41.0</td>\n",
       "      <td>2008-48</td>\n",
       "      <td>8.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145455</th>\n",
       "      <td>2017-06-21</td>\n",
       "      <td>Uluru</td>\n",
       "      <td>2.8</td>\n",
       "      <td>23.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.5</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1022.45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.0</td>\n",
       "      <td>2017-25</td>\n",
       "      <td>299.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145456</th>\n",
       "      <td>2017-06-22</td>\n",
       "      <td>Uluru</td>\n",
       "      <td>3.6</td>\n",
       "      <td>25.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.5</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1021.30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2017-25</td>\n",
       "      <td>299.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145457</th>\n",
       "      <td>2017-06-23</td>\n",
       "      <td>Uluru</td>\n",
       "      <td>5.4</td>\n",
       "      <td>26.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.5</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1018.90</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.0</td>\n",
       "      <td>2017-25</td>\n",
       "      <td>299.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145458</th>\n",
       "      <td>2017-06-24</td>\n",
       "      <td>Uluru</td>\n",
       "      <td>7.8</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.5</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1017.95</td>\n",
       "      <td>2.5</td>\n",
       "      <td>28.0</td>\n",
       "      <td>2017-25</td>\n",
       "      <td>299.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145459</th>\n",
       "      <td>2017-06-25</td>\n",
       "      <td>Uluru</td>\n",
       "      <td>14.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1019.05</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-26</td>\n",
       "      <td>432.142857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>145460 rows × 12 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 137
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T11:00:58.410053Z",
     "start_time": "2024-09-28T11:00:58.399344Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Apply fillna() on multiple columns at once using a dictionary\n",
    "merged_data.fillna({\n",
    "    'MinTemp': merged_data['MinTemp'].mean(),\n",
    "    'MaxTemp': merged_data['MaxTemp'].mean(),\n",
    "    'Rainfall': merged_data['Rainfall'].mean(),\n",
    "    'HumidityAve': merged_data['HumidityAve'].mean(),\n",
    "    'CloudAve': merged_data['CloudAve'].bfill(),\n",
    "    'PressureAve': merged_data['PressureAve'].mean(),\n",
    "    'Daily_Flu_Cases': merged_data['Daily_Flu_Cases'].mean(),\n",
    "}, inplace=True)"
   ],
   "id": "5e3b0df7b6a8f1d5",
   "outputs": [],
   "execution_count": 138
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **Cleaning The DataSet**",
   "id": "762ab0e97ca4defa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T11:00:58.466271Z",
     "start_time": "2024-09-28T11:00:58.457647Z"
    }
   },
   "cell_type": "code",
   "source": "merged_data\n",
   "id": "74a0dcccc87b1b13",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "             Date Location  MinTemp    MaxTemp  Rainfall  HumidityAve  \\\n",
       "0      2008-12-01   Albury     13.4  22.900000       0.6         46.5   \n",
       "1      2008-12-02   Albury      7.4  25.100000       0.0         34.5   \n",
       "2      2008-12-03   Albury     12.9  25.700000       0.0         34.0   \n",
       "3      2008-12-04   Albury      9.2  28.000000       0.0         30.5   \n",
       "4      2008-12-05   Albury     17.5  32.300000       1.0         57.5   \n",
       "...           ...      ...      ...        ...       ...          ...   \n",
       "145455 2017-06-21    Uluru      2.8  23.400000       0.0         37.5   \n",
       "145456 2017-06-22    Uluru      3.6  25.300000       0.0         38.5   \n",
       "145457 2017-06-23    Uluru      5.4  26.900000       0.0         38.5   \n",
       "145458 2017-06-24    Uluru      7.8  27.000000       0.0         37.5   \n",
       "145459 2017-06-25    Uluru     14.9  23.221348       0.0         49.0   \n",
       "\n",
       "        WindSpeedAve  PressureAve  CloudAve  WindGustSpeed Year_Week  \\\n",
       "0               22.0      1007.40       7.5           44.0   2008-48   \n",
       "1               13.0      1009.20       7.5           44.0   2008-48   \n",
       "2               22.5      1008.15       7.5           46.0   2008-48   \n",
       "3               10.0      1015.20       7.5           24.0   2008-48   \n",
       "4               13.5      1008.40       7.5           41.0   2008-48   \n",
       "...              ...          ...       ...            ...       ...   \n",
       "145455          12.0      1022.45       2.5           31.0   2017-25   \n",
       "145456          11.0      1021.30       2.5           22.0   2017-25   \n",
       "145457           9.0      1018.90       2.5           37.0   2017-25   \n",
       "145458          10.0      1017.95       2.5           28.0   2017-25   \n",
       "145459          17.0      1019.05       8.0            NaN   2017-26   \n",
       "\n",
       "        Daily_Flu_Cases  \n",
       "0              8.428571  \n",
       "1              8.428571  \n",
       "2              8.428571  \n",
       "3              8.428571  \n",
       "4              8.428571  \n",
       "...                 ...  \n",
       "145455       299.142857  \n",
       "145456       299.142857  \n",
       "145457       299.142857  \n",
       "145458       299.142857  \n",
       "145459       432.142857  \n",
       "\n",
       "[145460 rows x 12 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Location</th>\n",
       "      <th>MinTemp</th>\n",
       "      <th>MaxTemp</th>\n",
       "      <th>Rainfall</th>\n",
       "      <th>HumidityAve</th>\n",
       "      <th>WindSpeedAve</th>\n",
       "      <th>PressureAve</th>\n",
       "      <th>CloudAve</th>\n",
       "      <th>WindGustSpeed</th>\n",
       "      <th>Year_Week</th>\n",
       "      <th>Daily_Flu_Cases</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008-12-01</td>\n",
       "      <td>Albury</td>\n",
       "      <td>13.4</td>\n",
       "      <td>22.900000</td>\n",
       "      <td>0.6</td>\n",
       "      <td>46.5</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1007.40</td>\n",
       "      <td>7.5</td>\n",
       "      <td>44.0</td>\n",
       "      <td>2008-48</td>\n",
       "      <td>8.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2008-12-02</td>\n",
       "      <td>Albury</td>\n",
       "      <td>7.4</td>\n",
       "      <td>25.100000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.5</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1009.20</td>\n",
       "      <td>7.5</td>\n",
       "      <td>44.0</td>\n",
       "      <td>2008-48</td>\n",
       "      <td>8.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2008-12-03</td>\n",
       "      <td>Albury</td>\n",
       "      <td>12.9</td>\n",
       "      <td>25.700000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>22.5</td>\n",
       "      <td>1008.15</td>\n",
       "      <td>7.5</td>\n",
       "      <td>46.0</td>\n",
       "      <td>2008-48</td>\n",
       "      <td>8.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2008-12-04</td>\n",
       "      <td>Albury</td>\n",
       "      <td>9.2</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.5</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1015.20</td>\n",
       "      <td>7.5</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2008-48</td>\n",
       "      <td>8.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2008-12-05</td>\n",
       "      <td>Albury</td>\n",
       "      <td>17.5</td>\n",
       "      <td>32.300000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>57.5</td>\n",
       "      <td>13.5</td>\n",
       "      <td>1008.40</td>\n",
       "      <td>7.5</td>\n",
       "      <td>41.0</td>\n",
       "      <td>2008-48</td>\n",
       "      <td>8.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145455</th>\n",
       "      <td>2017-06-21</td>\n",
       "      <td>Uluru</td>\n",
       "      <td>2.8</td>\n",
       "      <td>23.400000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.5</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1022.45</td>\n",
       "      <td>2.5</td>\n",
       "      <td>31.0</td>\n",
       "      <td>2017-25</td>\n",
       "      <td>299.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145456</th>\n",
       "      <td>2017-06-22</td>\n",
       "      <td>Uluru</td>\n",
       "      <td>3.6</td>\n",
       "      <td>25.300000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.5</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1021.30</td>\n",
       "      <td>2.5</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2017-25</td>\n",
       "      <td>299.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145457</th>\n",
       "      <td>2017-06-23</td>\n",
       "      <td>Uluru</td>\n",
       "      <td>5.4</td>\n",
       "      <td>26.900000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.5</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1018.90</td>\n",
       "      <td>2.5</td>\n",
       "      <td>37.0</td>\n",
       "      <td>2017-25</td>\n",
       "      <td>299.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145458</th>\n",
       "      <td>2017-06-24</td>\n",
       "      <td>Uluru</td>\n",
       "      <td>7.8</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.5</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1017.95</td>\n",
       "      <td>2.5</td>\n",
       "      <td>28.0</td>\n",
       "      <td>2017-25</td>\n",
       "      <td>299.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145459</th>\n",
       "      <td>2017-06-25</td>\n",
       "      <td>Uluru</td>\n",
       "      <td>14.9</td>\n",
       "      <td>23.221348</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1019.05</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-26</td>\n",
       "      <td>432.142857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>145460 rows × 12 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 139
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **Training Weather and Dumping its pkl files**",
   "id": "22c7573ba230660a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T11:01:05.006452Z",
     "start_time": "2024-09-28T11:00:58.525050Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "\n",
    "# ensure 'Date' is in datetime format\n",
    "merged_data['Date'] = pd.to_datetime(merged_data['Date'])\n",
    "\n",
    "# Create 'DayOfYear' and 'DayOfWeek' features\n",
    "merged_data['DayOfYear'] = merged_data['Date'].dt.dayofyear\n",
    "merged_data['DayOfWeek'] = merged_data['Date'].dt.dayofweek\n",
    "\n",
    "# Create 'ChanceOfRain' based on 'Rainfall'\n",
    "merged_data['ChanceOfRain'] = np.where(merged_data['Rainfall'] > 3.3, 1, 0)\n",
    "\n",
    "# List to store each location's model file paths\n",
    "model_paths = []\n",
    "\n",
    "# splitting the dataset by location\n",
    "locations = merged_data['Location'].unique()\n",
    "\n",
    "for location in locations:\n",
    "    # filter data for the current location\n",
    "    location_data = merged_data[merged_data['Location'] == location]\n",
    "\n",
    "    # Selecting the features for temperature and rain prediction\n",
    "    features = ['MinTemp', 'MaxTemp', 'Rainfall', 'WindSpeedAve', 'HumidityAve', 'PressureAve', \n",
    "                'CloudAve', 'Daily_Flu_Cases', 'DayOfYear', 'DayOfWeek']\n",
    "\n",
    "    X = location_data[features]\n",
    "    y_min_temp = location_data['MinTemp']\n",
    "    y_max_temp = location_data['MaxTemp']\n",
    "    y_rain = location_data['ChanceOfRain']\n",
    "\n",
    "    # Using imputer to handle missing values\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "    # Standardizing the features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_imputed)\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train_temp, X_test_temp, y_train_min_temp, y_test_min_temp, y_train_max_temp, y_test_max_temp = train_test_split(\n",
    "        X_scaled, y_min_temp, y_max_temp, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    X_train_rain, X_test_rain, y_train_rain, y_test_rain = train_test_split(\n",
    "        X_scaled, y_rain, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # Train separate models for each location\n",
    "\n",
    "    # Linear Regression for MinTemp and MaxTemp\n",
    "    min_temp_model = LinearRegression()\n",
    "    max_temp_model = LinearRegression()\n",
    "    humidity_model = LinearRegression()\n",
    "\n",
    "    min_temp_model.fit(X_train_temp, y_train_min_temp)\n",
    "    max_temp_model.fit(X_train_temp, y_train_max_temp)\n",
    "    humidity_model.fit(X_train_rain, y_train_rain)\n",
    "\n",
    "    # Random Forest Classifier for ChanceOfRain\n",
    "    rain_model = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "    rain_model.fit(X_train_rain, y_train_rain)\n",
    "\n",
    "    #  Save each model to a .pkl file\n",
    "\n",
    "    # # Save models for the current location\n",
    "    # min_temp_model_file = f'min_temp_model_{location}.pkl'\n",
    "    # max_temp_model_file = f'max_temp_model_{location}.pkl'\n",
    "    # rain_model_file = f'rain_model_{location}.pkl'\n",
    "    # \n",
    "    # with open(min_temp_model_file, 'wb') as f:\n",
    "    #     pickle.dump(min_temp_model, f)\n",
    "    # \n",
    "    # with open(max_temp_model_file, 'wb') as f:\n",
    "    #     pickle.dump(max_temp_model, f)\n",
    "    # \n",
    "    # with open(rain_model_file, 'wb') as f:\n",
    "    #     pickle.dump(rain_model, f)\n",
    "    # \n",
    "    # # Append model file paths to the list\n",
    "    # model_paths.append({\n",
    "    #     'location': location,\n",
    "    #     'min_temp_model': min_temp_model_file,\n",
    "    #     'max_temp_model': max_temp_model_file,\n",
    "    #     'rain_model': rain_model_file\n",
    "    # })\n",
    "\n",
    "# Output model paths\n",
    "for model_info in model_paths:\n",
    "    print(f\"Models for {model_info['location']}:\")\n",
    "    print(f\"Min Temp Model: {model_info['min_temp_model']}\")\n",
    "    print(f\"Max Temp Model: {model_info['max_temp_model']}\")\n",
    "    print(f\"Rain Model: {model_info['rain_model']}\")\n",
    "\n",
    "print(len(model_paths))\n"
   ],
   "id": "67b72f0bdbc3c3fc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "execution_count": 140
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **Outputting the predictions**",
   "id": "b961d73dc4be9891"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T11:01:05.056734Z",
     "start_time": "2024-09-28T11:01:05.030335Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# Function to load models based on location\n",
    "def load_location_models(location):\n",
    "    min_temp_model_file = f'trained_model/min_temp_model_{location}.pkl'\n",
    "    max_temp_model_file = f'trained_model/max_temp_model_{location}.pkl'\n",
    "    rain_model_file = f'trained_model/rain_model_{location}.pkl'\n",
    "\n",
    "    with open(min_temp_model_file, 'rb') as file:\n",
    "        min_temp_model = pickle.load(file)\n",
    "        \n",
    "    with open(max_temp_model_file, 'rb') as file:\n",
    "        max_temp_model = pickle.load(file)\n",
    "        \n",
    "    with open(rain_model_file, 'rb') as file:\n",
    "        rain_model = pickle.load(file)\n",
    "\n",
    "    \n",
    "    return min_temp_model, max_temp_model, rain_model\n",
    "\n",
    "# Example location for prediction\n",
    "location = 'Melbourne'  # Replace with desired location\n",
    "\n",
    "# Load models for the specified location\n",
    "min_temp_model, max_temp_model, rain_model = load_location_models(location)\n",
    "\n",
    "\n",
    "# Prepare future date features\n",
    "future_dates = pd.date_range(start='2024-9-23', periods=7, freq='D')\n",
    "future_data = pd.DataFrame(future_dates, columns=['Date'])\n",
    "future_data['DayOfYear'] = future_data['Date'].dt.dayofyear\n",
    "future_data['DayOfWeek'] = future_data['Date'].dt.dayofweek\n",
    "\n",
    "# Get historical averages for relevant features\n",
    "historical_averages = merged_data.groupby('DayOfYear').agg({\n",
    "    'MinTemp': 'mean',\n",
    "    'MaxTemp': 'mean',\n",
    "    'Rainfall': 'mean',\n",
    "    'WindSpeedAve': 'mean',\n",
    "    'HumidityAve': 'mean',\n",
    "    'PressureAve': 'mean',\n",
    "    'CloudAve': 'mean',\n",
    "    'Daily_Flu_Cases': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "# Merge the historical averages with the future date features\n",
    "future_data = future_data.merge(historical_averages, on='DayOfYear', how='left')\n",
    "\n",
    "# Scale the features for future dates (convert to NumPy array to avoid feature names warning)\n",
    "X_future = future_data[['MinTemp', 'MaxTemp', 'Rainfall', 'WindSpeedAve', 'HumidityAve', 'PressureAve', 'CloudAve', 'Daily_Flu_Cases', 'DayOfYear', 'DayOfWeek']].values  # Convert to NumPy array\n",
    "\n",
    "# Apply the scaling (use the location-specific scaler from training)\n",
    "X_future_scaled = scaler.transform(X_future)\n",
    "\n",
    "# Predict ChanceOfRain using the trained Random Forest model\n",
    "y_future_rain_prob = rain_model.predict_proba(X_future_scaled)[:, 1]  # Probability of rain (class 1)\n",
    "\n",
    "# Convert probabilities to percentages and clip values to a valid range\n",
    "future_data['Predicted_ChanceOfRain'] = np.clip(y_future_rain_prob * 100, 0, 100)  # Convert to percentage\n",
    "\n",
    "# Predict MinTemp and MaxTemp using the trained Linear Regression models\n",
    "future_data['Predicted_Min_Temp'] = min_temp_model.predict(X_future_scaled)\n",
    "future_data['Predicted_Max_Temp'] = max_temp_model.predict(X_future_scaled)\n",
    "future_data['Predicted_Rain'] = rain_model.predict(X_future_scaled)\n",
    "\n",
    "# Display the predictions in the desired format\n",
    "predictions_output = future_data[['Date', 'Predicted_Min_Temp', 'Predicted_Max_Temp', 'Predicted_ChanceOfRain', 'Rainfall']]\n",
    "print(f\"\\nPredictions for future dates at {location}:\")\n",
    "print(predictions_output)\n"
   ],
   "id": "c33e9243587ff05",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predictions for future dates at Melbourne:\n",
      "        Date  Predicted_Min_Temp  Predicted_Max_Temp  Predicted_ChanceOfRain  \\\n",
      "0 2024-09-23            9.588005           14.528873                     2.0   \n",
      "1 2024-09-24            9.531679           14.388813                     3.0   \n",
      "2 2024-09-25            9.303901           14.090707                    99.0   \n",
      "3 2024-09-26            9.211739           14.693807                     0.0   \n",
      "4 2024-09-27            9.535896           15.007302                     0.0   \n",
      "5 2024-09-28            9.533444           14.614866                    97.0   \n",
      "6 2024-09-29            9.547825           14.665106                    10.0   \n",
      "\n",
      "   Rainfall  \n",
      "0  0.768480  \n",
      "1  2.214042  \n",
      "2  2.894813  \n",
      "3  1.251816  \n",
      "4  1.488369  \n",
      "5  2.779393  \n",
      "6  2.300841  \n"
     ]
    }
   ],
   "execution_count": 141
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T11:01:05.074082Z",
     "start_time": "2024-09-28T11:01:05.070484Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check for class imbalance in ChanceOfRain\n",
    "print(merged_data['ChanceOfRain'].value_counts(normalize=True))\n"
   ],
   "id": "99153df71fbc0aa7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChanceOfRain\n",
      "0    0.857301\n",
      "1    0.142699\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "execution_count": 142
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **TRAINING FLU**",
   "id": "4f49ec42630eb146"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T11:01:05.235200Z",
     "start_time": "2024-09-28T11:01:05.167836Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "\n",
    "# Preprocess the dataset: Drop rows with missing values\n",
    "df_cleaned = merged_data.dropna()\n",
    "\n",
    "# Feature selection for flu prediction\n",
    "X_flu = df_cleaned[['MinTemp', 'MaxTemp', 'Rainfall', 'WindSpeedAve', 'HumidityAve', 'PressureAve', 'CloudAve', 'DayOfYear', 'DayOfWeek']]\n",
    "y_flu = df_cleaned['Daily_Flu_Cases']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train_flu, X_test_flu, y_train_flu, y_test_flu = train_test_split(X_flu, y_flu, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Linear Regression model for flu prediction\n",
    "flu_model_lr = LinearRegression()\n",
    "\n",
    "# Train the model on the training set\n",
    "flu_model_lr.fit(X_train_flu, y_train_flu)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_flu_lr = flu_model_lr.predict(X_test_flu)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "mse_flu_lr = mean_squared_error(y_test_flu, y_pred_flu_lr)\n",
    "r2_flu_lr = r2_score(y_test_flu, y_pred_flu_lr)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse_flu_lr}\")\n",
    "print(f\"R-squared: {r2_flu_lr}\")\n",
    "\n",
    "# Future dates for prediction\n",
    "future_dates = pd.date_range(start='2024-9-01', periods=7, freq='D')\n",
    "\n",
    "# Create a DataFrame for future dates\n",
    "future_data = pd.DataFrame(future_dates, columns=['Date'])\n",
    "future_data['DayOfYear'] = future_data['Date'].dt.dayofyear\n",
    "future_data['DayOfWeek'] = future_data['Date'].dt.dayofweek\n",
    "\n",
    "# Use historical averages for weather data\n",
    "historical_averages = df_cleaned.groupby('DayOfYear').agg({\n",
    "    'MinTemp': 'mean',\n",
    "    'MaxTemp': 'mean',\n",
    "    'Rainfall': 'mean',\n",
    "    'WindSpeedAve': 'mean',\n",
    "    'HumidityAve': 'mean',\n",
    "    'PressureAve': 'mean',\n",
    "    'CloudAve': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "# Merge the historical averages with the future date features\n",
    "future_data = future_data.merge(historical_averages, on='DayOfYear', how='left')\n",
    "\n",
    "# Select features for prediction (ensure it matches the training features)\n",
    "X_future = future_data[['MinTemp', 'MaxTemp', 'Rainfall', 'WindSpeedAve', 'HumidityAve', 'PressureAve', 'CloudAve', 'DayOfYear', 'DayOfWeek']]\n",
    "\n",
    "# Predict flu cases using the trained model\n",
    "future_data['Predicted_Flu_Cases'] = flu_model_lr.predict(X_future)\n",
    "\n",
    "# Define thresholds for flu risk categories based on predicted flu cases\n",
    "def classify_flu_risk(predicted_cases):\n",
    "    if predicted_cases < 20:\n",
    "        return \"Low Risk\"\n",
    "    elif 20 <= predicted_cases <= 50:\n",
    "        return \"Moderate Risk\"\n",
    "    else:\n",
    "        return \"High Risk\"\n",
    "\n",
    "# Apply the classification to the predicted flu cases\n",
    "future_data['Flu_Risk_Category'] = future_data['Predicted_Flu_Cases'].apply(classify_flu_risk)\n",
    "\n",
    "# Display the results with flu risk categories\n",
    "print(future_data[['Date', 'Predicted_Flu_Cases', 'Flu_Risk_Category']])\n"
   ],
   "id": "25f98ac064a6c646",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 52787.04538903461\n",
      "R-squared: 0.11752265145113172\n",
      "        Date  Predicted_Flu_Cases Flu_Risk_Category\n",
      "0 2024-09-01           213.959917         High Risk\n",
      "1 2024-09-02           208.527742         High Risk\n",
      "2 2024-09-03           205.588476         High Risk\n",
      "3 2024-09-04           209.875833         High Risk\n",
      "4 2024-09-05           209.596020         High Risk\n",
      "5 2024-09-06           209.095585         High Risk\n",
      "6 2024-09-07           211.900645         High Risk\n"
     ]
    }
   ],
   "execution_count": 143
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T11:01:05.359478Z",
     "start_time": "2024-09-28T11:01:05.327081Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'merged_data' contains both weather and flu data, merged on a weekly basis\n",
    "# Make sure 'merged_data' has a 'Date' column\n",
    "X = merged_data[['Date', 'MinTemp', 'MaxTemp', 'Rainfall', 'HumidityAve']]  # Weather features + Date\n",
    "y = merged_data['Daily_Flu_Cases']  # Target variable (flu cases)\n",
    "\n",
    "# Split the data into training and testing sets, while keeping track of the 'Date' column\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Separate the 'Date' column for the test set to later merge with predictions\n",
    "dates_test = X_test['Date']\n",
    "\n",
    "# Drop the 'Date' column from X_train and X_test for model training\n",
    "X_train = X_train.drop(columns=['Date'])\n",
    "X_test = X_test.drop(columns=['Date'])\n",
    "\n",
    "# Initialize the Linear Regression model\n",
    "flu_model = LinearRegression()\n",
    "\n",
    "# Train the model\n",
    "flu_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = flu_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model using metrics such as Mean Squared Error (MSE) and R-squared\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-Squared: {r2}\")\n",
    "\n",
    "# Convert predictions into probabilities (based on the threshold)\n",
    "threshold_value = 256  # Flu outbreak threshold\n",
    "y_pred_proba = (y_pred > threshold_value).astype(float)  # Probability of a flu outbreak\n",
    "\n",
    "# Convert to percentage of chance for an outbreak\n",
    "y_pred_outbreak_percentage = np.clip(y_pred_proba * 100, 0, 100)\n",
    "\n",
    "# Combine the 'Date' and predictions into a DataFrame for easy output\n",
    "predictions_by_date = pd.DataFrame({\n",
    "    'Date': dates_test,\n",
    "    'Predicted_Flu_Cases': y_pred,\n",
    "    'Predicted_Outbreak_Chance(%)': y_pred_outbreak_percentage\n",
    "})\n",
    "\n",
    "# Display predictions with corresponding dates\n",
    "print(\"\\nPredictions by Date:\")\n",
    "print(predictions_by_date)\n"
   ],
   "id": "ab26d0df7aece162",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 53902.78740708362\n",
      "R-Squared: 0.09894800372782175\n",
      "\n",
      "Predictions by Date:\n",
      "             Date  Predicted_Flu_Cases  Predicted_Outbreak_Chance(%)\n",
      "100721 2012-04-22           155.844343                           0.0\n",
      "30234  2008-03-30           131.435033                           0.0\n",
      "68427  2011-12-10            81.304043                           0.0\n",
      "28624  2013-03-27            39.998255                           0.0\n",
      "31173  2010-10-25           149.130368                           0.0\n",
      "...           ...                  ...                           ...\n",
      "57136  2014-09-24           178.678291                           0.0\n",
      "30386  2008-08-29           197.748667                           0.0\n",
      "17904  2016-11-26           120.799972                           0.0\n",
      "128313 2013-04-23           142.199415                           0.0\n",
      "80574  2016-04-28            96.178894                           0.0\n",
      "\n",
      "[29092 rows x 3 columns]\n"
     ]
    }
   ],
   "execution_count": 144
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T11:01:05.470035Z",
     "start_time": "2024-09-28T11:01:05.463823Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "fce97edfc8417e9d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
